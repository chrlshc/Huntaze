Codebase Cleanup and Refactoring: Architectural Implementation of Analysis Phase Tools and Data Models1. Strategic Context and Architectural VisionThe modernization of legacy software systems represents one of the most persistent challenges in contemporary software engineering. As codebases expand, they invariably succumb to entropy—a gradual decline in disorder manifested as dead code, duplicated logic, architectural drift, and fragmented documentation. This phenomenon, quantified as Technical Debt Ratio (TDR), directly impedes velocity and innovation.1 The "Codebase Cleanup and Refactor" initiative is not merely a maintenance task; it is a strategic architectural intervention designed to arrest this entropy and establish a foundation for automated, AI-assisted evolution.The implementation of this initiative rests on a fundamental axiom: distinct observability must precede modification. One cannot safely refactor a system one cannot measure. Consequently, the Analysis Phase is the bedrock of the entire operation. This report details the comprehensive implementation of the Analysis Phase tools—specifically the File Scanner, CSS Analyzer, and their underlying Data Models.1.1 The Role of AI Agents in RefactoringA critical driver for this architectural overhaul is the integration of AI coding agents into the development lifecycle. Recent studies indicate that while agents like OpenAI Codex or Devin excel at routine maintenance and localized refactoring, they lack deep architectural situational awareness.2 When tasked with broad architectural changes, these agents often function as "expensive autocomplete," struggling to understand the ripple effects of their changes across a complex dependency graph.2To bridge this gap, the Analysis Phase tools are designed to provide the deterministic context that probabilistic AI models require. By constructing a rigorous, mathematically sound graph of the codebase, we transform the refactoring process from a heuristic guess into a precision engineering operation. The tools defined herein act as the sensory organ for the AI, feeding it structured data about duplicates, dependency cycles, and style redundancies that a pure Large Language Model (LLM) context window could never encompass.1.2 The Technical Debt LandscapeTechnical debt is often discussed in abstract terms, but for the purpose of this implementation, we define it through measurable artifacts that our tools will detect.Debt CategoryDetectable ArtifactArchitectural ImpactCode BloatUnreachable Files / Dead ExportsIncreases build times; confuses developers; bloats bundle size.Style EntropyDuplicate CSS SelectorsIncreases layout thrashing; causes "specificity wars"; breaks visual consistency.3Documentation DriftRedundant/Conflicting .md filesMisleads onboarding developers; creates "split-brain" knowledge bases.Cyclic DependenciesGraph CyclesPrevents tree-shaking; complicates testing; causes runtime initialization errors.4The architecture proposed utilizes a tiered analysis pipeline. It begins with physical file scanning to establish ground truth, moves to syntactic analysis (AST parsing) to understand structure, and concludes with semantic analysis (dependency graphs and NLP) to understand intent. This report provides the blueprint for implementing this pipeline using high-performance Node.js patterns.2. Comprehensive Data Model DesignThe efficacy of any static analysis toolchain relies heavily on the fidelity and extensibility of its data models. In the context of automated refactoring, the data model must bridge the gap between the physical file system and the logical application structure. We define a tri-partite data architecture: the FileSystemNode, the DependencyGraph, and the StyleSheetModel.2.1 The FileSystemNode EntityThe FileSystemNode is the atomic unit of the analysis phase. It abstracts the properties of a file on disk into a structure suitable for graph operations. Unlike standard fs.Stats objects in Node.js, the FileSystemNode is enriched with content-addressable identity (hashes) and AST-derived metadata. This decoupling of "identity" from "location" is crucial for tracking files as they are moved during refactoring.2.1.1 Schema DefinitionThe schema is designed to support both rapid change detection (via mtime and size) and deep content analysis (via ast and hash).PropertyTypeDescriptionRationaleidUUIDUnique IdentifierDecouples the node from its filepath, allowing file moves without graph regeneration.pathStringRelative File PathUsed for display and disk operations.hashString (SHA-256)Content ChecksumEnables duplicate detection and cache invalidation.5mtimeIntegerModification TimestampHeuristics for incremental scanning.6sizeIntegerFile Size (Bytes)Metric for identifying "bloat" candidates.astObject (Lazy)Abstract Syntax TreeCached AST for subsequent analysis phases (e.g., PostCSS Root).importsArrayOutgoing EdgesList of resources this file depends on.exportsArrayExposed SymbolsList of functions/classes/variables exported.2.1.2 Cryptographic Integrity and Hashing StrategiesA critical architectural decision is the selection of the hashing algorithm for content addressability. Historically, MD5 has been the standard for file integrity checks due to its speed. However, MD5 is no longer considered collision-resistant.5 In a large-scale deduplication system, a collision (two different files producing the same hash) could lead to catastrophic data loss if the system automatically deletes the "duplicate."Therefore, this implementation mandates the use of SHA-256. While slightly more computationally expensive than MD5 or SHA-1, SHA-256 offers a collision resistance that effectively eliminates the risk of accidental data deletion.5 Modern Node.js versions (utilizing OpenSSL) provide highly optimized implementations of SHA-256 that minimize the performance overhead. Furthermore, the model incorporates a "lazy-hash" strategy: the hash is only computed when the mtime or size differs from the cached version. This is crucial for performance; frequent re-hashing of large binaries (e.g., images or video assets) can saturate the CPU event loop.72.2 The DependencyGraph ArchitectureThe DependencyGraph is a directed graph where vertices represent FileSystemNode entities and edges represent import/require relationships. This model is essential for identifying "dead code" (nodes with an in-degree of zero, excluding entry points) and "orphaned subgraphs" (isolated clusters of nodes).42.2.1 Graph Topology and Cyclic DependenciesThe model must explicitly handle cyclic dependencies (e.g., A imports B, B imports A). In large legacy codebases, cycles are common and can prevent standard garbage collection algorithms from identifying unused code. A standard reference counting approach will fail in the presence of cycles because the members of the cycle maintain non-zero reference counts to each other, even if the entire group is detached from the application entry point.Implementation Strategy:The DependencyGraph utilizes a customized Tarjan’s algorithm or Kosaraju's algorithm to detect Strongly Connected Components (SCCs). By treating an SCC as a single "super-node" in the graph, we can perform topological sorting and reachability analysis. If an entire SCC is unreachable from the defined entry points (e.g., index.js, App.js), the entire component is flagged for removal. This approach is superior to simple reference counting as it enables the deletion of entire subgraphs of dead code simultaneously, accelerating the cleanup process.42.3 The StyleSheetModelFor CSS analysis, a specialized model is required to represent the cascade and specificity of style rules. The StyleSheetModel extends the FileSystemNode but parses the content into a CSS Object Model (CSSOM) or AST via PostCSS.Key Attributes:selectors: A map of selector strings to their occurrence counts and source lines. This allows us to identify how many times .button-primary is defined across the codebase.mediaQueries: A structured list of @media blocks, allowing for the analysis of mobile-vs-desktop rule segregation.8duplicates: A computed list of rules where the selector and property-value pairs are identical to other rules in the project.This model serves as the input for the CSS Analyzer, enabling the detection of "spaghetti code" where styles are overridden multiple times or duplicated across chunk files, leading to unpredictable visual regressions.33. Tool 1: High-Performance File Scanner ImplementationThe File Scanner is the ingestion engine of the analysis phase. Its performance dictates the user experience; a slow scan interrupts the "flow" of the developer and discourages frequent use of the cleanup tools. The architecture defined here moves beyond standard recursive crawling to utilize high-speed, non-blocking I/O operations and advanced concurrency patterns.3.1 Directory Traversal ArchitectureScanning a directory seems trivial, but at the scale of modern monorepos (often exceeding 100,000 files), naive implementations collapse under the weight of I/O latency and object allocation.3.1.1 The Limitations of Native RecursionTraditional recursive approaches using fs.readdir or glob are often bottlenecks in Node.js. glob libraries, in particular, can be an order of magnitude slower than native traversal due to the overhead of string pattern matching and the internal generation of Promise objects.9 Furthermore, synchronous recursion (readdirSync) blocks the Node.js event loop, preventing other operations (like hashing or UI updates) from proceeding, while uncontrolled asynchronous recursion can exhaust file descriptors (EMFILE errors).103.1.2 Optimization Strategy: fdir vs. globTo achieve maximum throughput, this implementation utilizes a high-performance crawler architecture similar to fdir, which benchmarks significantly faster than glob, recursive-fs, or native recursive implementations.11Key Architectural Decisions:Iterative Traversal: Instead of the call-stack-heavy recursive approach, we employ an iterative stack-based traversal. This prevents RangeError: Maximum call stack size exceeded on deeply nested directories.Object Allocation Reduction: fdir and similar high-performance crawlers minimize the creation of intermediate objects. Paths are constructed using string concatenation rather than path.join (which has higher overhead) where safe.Concurrency Control: While we want parallelism, we must respect the operating system's limits. The scanner uses a p-limit style concurrency queue to keep active file descriptors within system limits (typically 1024 or 4096 on POSIX systems) while maximizing disk throughput.3.2 Duplicate Detection EngineDuplicate files—whether accidental copies of assets, versioned backups (e.g., logo_final_v2.png), or copy-pasted utility files—are a primary source of storage bloat and confusion.3.2.1 Streaming Hash CalculationTo detect duplicates, the scanner computes the SHA-256 hash of every file. A naive implementation might attempt to load the file into a buffer (fs.readFile) and then hash it. For large files (e.g., video assets or large datasets), this spikes the Resident Set Size (RSS) memory usage, potentially crashing the Node.js process or triggering aggressive Garbage Collection cycles that degrade performance.7Implementation:The scanner employs Node.js Streams and the crypto module pipeline. This approach allows data to flow from the disk to the hash function in small chunks (typically 64KB), keeping memory usage constant regardless of file size.13Logic Flow:Read Stream: A stream is opened for the file (fs.createReadStream).Transform Stream: The data is piped into crypto.createHash('sha256').Pipeline Management: We use stream.promises.pipeline to manage the flow and handle errors. This utility function ensures that streams are properly destroyed if an error occurs, preventing file descriptor leaks.133.2.2 The Two-Pass Optimization StrategyComputing a cryptographic hash for every single file in a large project is CPU-intensive. To optimize this, the scanner implements a two-pass deduction strategy.14Pass 1 (Metadata): The scanner traverses the file system and groups files by their file size (in bytes).Pass 2 (Content): Hash calculation is triggered only for groups where more than one file shares the exact same size.If files_with_size_X.length == 1, that file is unique by definition (no other file has that byte count).If files_with_size_X.length > 1, we compute the hashes for these specific files to verify if their content is identical.This heuristic drastically reduces the CPU time required, as the vast majority of source files (code) have unique byte counts. Collisions in file size are rare for source code, though more common for generated assets.3.3 Integration with Version ControlThe scanner must respect .gitignore rules to avoid analyzing build artifacts (node_modules, dist, coverage, .git). Scanning these directories introduces massive noise and performance degradation.15 The scanner parses the .gitignore file at the root using the ignore package and constructs a filter predicate that excludes matching paths during the traversal phase, ensuring that the DependencyGraph is not polluted with third-party library code.4. Tool 2: CSS Analyzer & Optimizer ImplementationRefactoring CSS is notoriously difficult due to the global nature of the cascade. A change in one file can cause visual regressions in unrelated components. The CSS Analyzer provides the safety net required to refactor styles by building a precise model of style definitions, usages, and duplications.4.1 AST Parsing with PostCSSThe analyzer leverages PostCSS to transform raw CSS files into an Abstract Syntax Tree (AST). PostCSS is chosen over regex-based parsing or other preprocessors because of its speed, modular plugin architecture, and robust API for AST manipulation.164.1.1 The PostCSS PipelineThe analysis script configures a PostCSS processor with custom plugins designed for inspection rather than transformation.Input: The raw CSS string and its source map.Parser: PostCSS generates a Root node containing Rules, AtRules (like @media), and Declarations.Walker: A custom walker iterates through the AST, populating the StyleSheetModel. It inspects root.source.input.file to link styles back to their physical location 18, enabling the report to generate actionable links (e.g., "Duplicate rule found in src/components/Header.css:42").4.2 Duplicate Selector DetectionA common pattern in legacy codebases is the re-declaration of the same selector with identical properties, often due to copy-pasting or poor merge conflict resolution.4.2.1 Exact and Partial Match DetectionThe analyzer implements a robust comparison algorithm that goes beyond simple string matching:Normalization: Selectors are sorted and normalized (e.g., .a,.b is treated as identical to .b,.a).Property Sorting: Declarations within a rule are sorted alphabetically to ensure that color: red; margin: 0; matches margin: 0; color: red;.Fingerprinting: A hash of the normalized rule is computed. If the hash exists in the global index, a duplicate is flagged.19This logic mirrors the behavior of plugins like postcss-combine-duplicated-selectors, but instead of automatically merging them (which can be risky regarding cascade order), the Analyzer reports them for review. Furthermore, the tool identifies "partial duplicates"—rules that target the same selector but have different values. These are critical for identifying "Specificity Wars," where a developer has overridden a style simply to win a specificity battle.34.3 Media Query Extraction and AnalysisModern responsive design often leads to bloated stylesheets where mobile users are forced to download desktop-specific CSS. The CSS Analyzer includes a module to audit Media Queries.4.3.1 Query ClassificationThe analyzer parses @media parameters using css-mediaquery to validate and classify them into buckets (e.g., "Mobile", "Tablet", "Desktop").204.3.2 Extraction StrategyThe tool identifies large blocks of CSS wrapped in desktop-only queries (e.g., @media (min-width: 1024px)). It calculates the percentage of the CSS file devoted to these queries. If a critical threshold (e.g., 30%) is exceeded, the Analyzer recommends splitting the file. This aligns with the performance best practice of extracting media queries into separate files (desktop.css, mobile.css) to improve the Critical Rendering Path.8 By loading desktop styles only when needed, the Time to Interactive (TTI) for mobile users is significantly reduced.4.4 Automated CSS Cleanup PromptsWhile AI agents struggle with architectural refactoring, they excel at localized cleanup.2 The CSS Analyzer leverages its findings to generate context-aware prompts for AI assistants.Prompt Generation: "Review the CSS rule .header-nav in src/layout.css. It shares 90% similarity with .footer-nav in src/footer.css. Refactor these into a shared utility class."This targeted approach bypasses the "timid" nature of AI refactoring tools when faced with generic "clean this up" prompts.225. Tool 3: Advanced Unused Code & Dependency AnalysisIdentifying dead code is the most direct way to reduce technical debt. The Dependency Analyzer builds upon the DependencyGraph to perform reachability analysis, but it requires sophisticated parsing to handle the nuances of TypeScript and JavaScript modules.5.1 AST-Based Import DetectionRegex-based searching for imports is insufficient due to the complexity of modern JavaScript (dynamic imports, aliases, re-exports). The tool uses ts-morph, a wrapper around the TypeScript Compiler API, to generate a precise dependency graph.235.1.1 Handling TypeScript Exports and Re-exportsTools like knip or ts-prune are integrated into the architecture to detect unused exports. Standard linters (like ESLint) detect unused variables within a file, but they cannot see if an exported function is never imported by another file in the project.25Workflow:Project Loading: The analyzer loads the tsconfig.json to understand path aliases and root directories.Symbol Tracking: It iterates through all source files, collecting all exported symbols.Reference Counting: It scans all imports. If an exported symbol has zero references across the entire project, it is flagged as a "Dead Export".Entry Point Exclusion: The tool accepts a configuration of "entry points" (e.g., main.ts, pages/*, test/*) which are immune to pruning.5.1.2 Orphan DetectionBeyond individual symbols, the analyzer looks for "Orphans"—files that are never imported by any other file. While this sounds simple, it requires a complete graph traversal. The tool employs the no-orphans rule logic found in dependency-cruiser to identify these isolated files.26 It is critical to differentiate between true orphans and implicit entry points (like Next.js pages or serverless handlers), which necessitates a robust configuration allowing for "whitelist" patterns.5.2 Redundant Documentation DetectionCodebases often accumulate documentation that is out of sync or redundant. A project might have SETUP.md, INSTALL.md, and docs/getting-started.md, all containing variations of the same instructions. The analysis toolkit includes a Documentation Scanner.5.2.1 Semantic Similarity with TF-IDFTo find duplicate or highly similar Markdown files, the tool implements a Term Frequency-Inverse Document Frequency (TF-IDF) algorithm.27 This is a statistical measure used to evaluate how important a word is to a document in a collection.Implementation Steps:Tokenization: Markdown files are parsed using remark-parse.29 Text content is extracted, and code blocks are treated as separate tokens or ignored depending on configuration.30Vectorization: Each document is converted into a vector of term weights. Rare terms (like specific API names) carry more weight than common terms (like "the" or "and").Cosine Similarity: The scanner computes the cosine similarity between document vectors.Thresholding: Pairs with similarity > 0.85 are flagged as candidates for consolidation.This approach is superior to simple string matching (Levenshtein distance) for long documents because it captures the semantic structure and key terms rather than just character differences. It allows the system to identify that setup.md and installation.md are effectively the same document even if the wording is slightly different.316. Testing Strategy: Property-Based ValidationBuilding tools that delete code requires an extraordinarily high confidence level. A bug in the scanner could result in the deletion of critical production code. To mitigate this, we employ Property-Based Testing (PBT) using fast-check.6.1 The Philosophy of Model-Based TestingStandard unit tests check specific examples (e.g., "scan directory A, expect file B"). PBT checks properties that must always hold true (e.g., "scanning any directory structure should never crash" or "the hash of a file should be invariant regardless of its location").326.2 Implementation with Fast-Check and MemfsTo test the File Scanner without thrashing the physical disk (and to ensure tests run in isolation), we utilize memfs, an in-memory file system simulation that effectively mocks the Node.js fs module.346.2.1 Generator ArbitrariesWe define custom fast-check arbitraries that generate random, deeply nested directory structures. These arbitraries are capable of producing edge cases that humans rarely test for, such as circular symlinks, empty files, filenames with zero-width characters, and maximum path length violations.36Test Scenario Example:We define a property: "For any generated directory tree structure T, the scanner must return a node count exactly equal to the number of files in T."fast-check will then generate hundreds of random variations of directory trees (deep, wide, empty, complex), instantiate them in memfs, run the scanner, and verify the count. If a failure is found (e.g., the scanner double-counts a symlink), fast-check minimizes the test case to the smallest possible reproduction (shrinking).336.2.2 State Machine Testing for Graph IntegrityWe verify the robustness of the DependencyGraph using state machine testing. We model the system as a state (the graph) and apply random operations (add file, delete file, modify import). After a sequence of random operations, we verify invariants, such as "no edge should point to a non-existent node".37 This rigorous approach ensures the tool handles complex refactoring scenarios (e.g., rapid file moves or git checkouts) without corrupting the graph state.7. Implementation Roadmap and ScriptsThe following section outlines the concrete implementation steps and logic for the scripts defined in the design document.7.1 Script Logic: scan-files.tsObjective: Traverse directory, compute hashes, build initial FileNode list.Dependencies: fdir, crypto, ignore, p-limit.Algorithm:Initialize Crawler: Configure fdir with withFullPaths() and withErrors() (to handle permission issues gracefully).Filter Stage: Before hashing, filter the file list against the loaded .gitignore instance.Size-Grouping: Create a Map<number, FileNode> where the key is the file size.Hash Queue: Iterate over the map.If FileNode.length === 1: Unique file. Skip hashing (unless hash is required for cache keys).If FileNode.length > 1: Push to hashingQueue.Concurrency: Process hashingQueue with p-limit (limit 4).Output: Write files.json containing the registry of nodes.7.2 Script Logic: analyze-css.tsObjective: Parse CSS, detect duplicates, extract metrics.Dependencies: postcss, postcss-value-parser, css-mediaquery.Algorithm:Load Files: Read all .css (or .scss/.less) files identified by scan-files.ts.Parse: postcss.parse(css, { from: filePath }).Rule Traversal: root.walkRules(rule => {... }).Fingerprinting:Normalize selector: selector.replace(/\s+/g, ' ').trim().Sort declarations: rule.nodes.sort((a,b) => a.prop.localeCompare(b.prop)).Generate signature: normSelector + '{' + sortedDecls + '}'.Registry Update: Add signature to GlobalStyleMap.If exists: Record duplicate (file path, line number).Media Query Check: root.walkAtRules('media', rule => {... }).Parse params.Accumulate character count of the rule's contents.Update MediaMetrics (e.g., { "min-width: 1024px": 4500 bytes }).7.3 Script Logic: find-dead-code.tsObjective: Identify unused exports and files.Dependencies: ts-morph, knip.Algorithm:Graph Construction: Initialize ts-morph Project.Symbol Collection: sourceFile.getExportedDeclarations(): Map all exports.Reference Hunting:For each export, call Node.findReferences().Optimization: Use languageService.getFileReferences if possible, or restrict search scope.Pruning:Nodes with 0 incoming edges (excluding Entry Points) are Dead Files.Exports with 0 references are Dead Exports.7.4 Validation: test-scanner.spec.tsObjective: Verify scanner behaves correctly under chaos.Dependencies: vitest, fast-check, memfs.Logic:TypeScripttest('Scanner handles random file structures', async () => {
  await fc.assert(
    fc.asyncProperty(fc.object({ maxDepth: 5 }), async (treeConfig) => {
      vol.reset();
      const jsonTree = generateRandomTree(treeConfig); // Helper using arbitraries
      vol.fromJSON(jsonTree);
      
      const nodes = await scanner.run({ root: '/' });
      
      // Assertion: Number of nodes matches files created in memfs
      expect(nodes.length).toBe(Object.keys(jsonTree).length);
    })
  );
});
8. ConclusionThe architecture proposed herein transforms the abstract concept of "codebase cleanup" into a quantifiable, engineering-driven process. By moving beyond simple linters to deep AST analysis and graph theory, we gain the ability to visualize the "dark matter" of our software—the dead code, the duplicated logic, and the structural inefficiencies that slow down development.The combination of stream-based scanning for performance, PostCSS AST analysis for precision, and property-based testing for reliability creates a toolchain that is safe enough to run in CI/CD pipelines yet powerful enough to guide massive architectural refactors. This Analysis Phase is not merely a cleanup task; it is the deployment of a persistent immune system for the codebase, ensuring that as the organization scales, its technical debt remains strictly managed.