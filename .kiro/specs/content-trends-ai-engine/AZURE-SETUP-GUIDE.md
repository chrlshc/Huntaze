# Content Trends AI Engine - Azure AI Foundry Setup Guide

## üéØ Quick Start

Tu as d√©j√† DeepSeek-R1 d√©ploy√©. Voici comment activer le pipeline complet avec la nouvelle stack Phi-4 + Azure Speech.

## üìã Mod√®les Requis

| Mod√®le | Usage | Pricing |
|--------|-------|---------|
| **DeepSeek-R1** | Analyse virale, raisonnement | $0.00135/1K in, $0.0054/1K out |
| **DeepSeek-V3** | G√©n√©ration (hooks, scripts) | $0.00114/1K in, $0.00456/1K out |
| **Phi-4-multimodal-instruct** | Analyse frames + audio (128K context) | ~$0.0004/1K tokens |
| **Azure Speech Batch** | Transcription audio | $0.18/heure |

### Migration depuis Llama Vision

Le syst√®me utilise maintenant **Phi-4-multimodal-instruct** au lieu de Llama 3.2 Vision:
- ‚úÖ Contexte 128K (vs 128K Llama)
- ‚úÖ Analyse unifi√©e texte + images + audio en un seul appel
- ‚úÖ Disponible via Azure Foundry Partners & Community
- ‚úÖ Support natif de l'analyse "timeline seconde par seconde"

## üîß Configuration .env.local

```bash
# DeepSeek R1 - Tu l'as d√©j√†!
AZURE_DEEPSEEK_R1_ENDPOINT=https://ton-endpoint-r1.eastus2.models.ai.azure.com
AZURE_DEEPSEEK_R1_DEPLOYMENT=deepseek-r1-reasoning

# DeepSeek V3 - √Ä d√©ployer sur Azure Marketplace
AZURE_DEEPSEEK_V3_ENDPOINT=https://ton-endpoint-v3.eastus2.models.ai.azure.com
AZURE_DEEPSEEK_V3_DEPLOYMENT=deepseek-v3-generation

# Phi-4 Multimodal - NOUVEAU (remplace Llama Vision)
AZURE_PHI4_MULTIMODAL_ENDPOINT=https://ton-endpoint-phi4.eastus2.models.ai.azure.com
AZURE_PHI4_MULTIMODAL_DEPLOYMENT=phi-4-multimodal-instruct

# Azure Speech Batch Transcription - NOUVEAU
AZURE_SPEECH_ENDPOINT=https://eastus2.api.cognitive.microsoft.com
AZURE_SPEECH_KEY=REDACTED-cle-speech
AZURE_SPEECH_REGION=eastus2

# Llama Vision (LEGACY - fallback si Phi-4 non configur√©)
AZURE_LLAMA_VISION_ENDPOINT=https://ton-endpoint-vision.eastus2.models.ai.azure.com
AZURE_LLAMA_VISION_DEPLOYMENT=llama-32-vision

# Cl√© API partag√©e
AZURE_AI_API_KEY=REDACTED-cle-api

# R√©gion
AZURE_AI_REGION=eastus2
```

## üöÄ D√©ployer les mod√®les sur Azure

### 1. DeepSeek-V3 (si pas encore d√©ploy√©)

```bash
# Via Azure Portal
1. Azure AI Foundry ‚Üí Model Catalog
2. Chercher "DeepSeek-V3"
3. Deploy ‚Üí Serverless API
4. Copier l'endpoint et la cl√©
```

### 2. Phi-4-multimodal-instruct (NOUVEAU)

```bash
# Via Azure Portal
1. Azure AI Foundry ‚Üí Model Catalog ‚Üí Partners & Community
2. Chercher "Phi-4-multimodal-instruct"
3. Deploy ‚Üí Serverless API
4. Copier l'endpoint et la cl√©
```

**Avantages Phi-4:**
- Analyse unifi√©e texte + images + audio via Chat Completions
- Contexte 128K pour analyse compl√®te de shorts
- Meilleure compr√©hension du contexte multimodal

### 3. Azure Speech Batch Transcription (NOUVEAU)

```bash
# Via Azure Portal
1. Azure Portal ‚Üí Create Resource ‚Üí Speech Services
2. Choisir r√©gion (eastus2 recommand√©)
3. Copier la cl√© et l'endpoint
4. Configurer dans .env.local
```

**Pricing:** $0.18/heure de transcription batch
- Id√©al pour traitement en volume
- Speaker diarization inclus
- Timestamps align√©s pour timeline analysis

## üì° Tester la Configuration

```bash
# V√©rifier la config
npx ts-node scripts/test-content-trends-config.ts

# Ou via l'API
curl http://localhost:3000/api/ai/content-trends/analyze
```

## üé¨ Pipeline d'Analyse Vid√©o (Mis √† jour)

```
Vid√©o ‚Üí FFmpeg (1 fps) ‚Üí Phi-4 Multimodal ‚Üí DeepSeek R1 ‚Üí DeepSeek V3
  ‚Üì           ‚Üì                  ‚Üì                ‚Üì              ‚Üì
Audio ‚Üí Azure Speech      Timeline JSON     Viral Score    Hooks/Scripts
         ($0.18/h)         + Audio Context
```

### Nouveau: Analyse Timeline Seconde par Seconde

Le pipeline permet maintenant une analyse "timeline seconde par seconde" pour les shorts:
1. **Extraction keyframes** - FFmpeg extrait les frames cl√©s
2. **Transcription audio** - Azure Speech transcrit avec timestamps
3. **Analyse unifi√©e** - Phi-4 corr√®le frames + audio
4. **Diagnostic viral** - DeepSeek R1 identifie les m√©canismes
5. **G√©n√©ration** - DeepSeek V3 produit hooks et scripts

### Exemple d'appel API

```bash
curl -X POST http://localhost:3000/api/ai/content-trends/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "frameUrls": [
      "https://storage.blob.core.windows.net/frames/frame-0.jpg",
      "https://storage.blob.core.windows.net/frames/frame-1.jpg"
    ],
    "audioUrl": "https://storage.blob.core.windows.net/audio/video-audio.wav",
    "transcript": "Hey guys, check this out...",
    "engagementMetrics": {
      "views": 100000,
      "likes": 5000,
      "shares": 200,
      "comments": 150
    },
    "brandContext": {
      "industry": "fitness",
      "tone": "energetic",
      "targetAudience": "18-35 fitness enthusiasts"
    },
    "analysisOptions": {
      "timelineAnalysis": true,
      "secondBySecond": true
    }
  }'
```

### R√©ponse (avec Timeline Analysis)

```json
{
  "success": true,
  "data": {
    "timeline": [
      {
        "second": 0,
        "action": "Person appears with surprised expression",
        "audioContent": "Hey guys, check this out",
        "emotions": ["surprise", "excitement"],
        "textOnScreen": "WAIT FOR IT...",
        "hookScore": 92,
        "patternInterrupts": ["zoom_in", "text_overlay"],
        "engagementPeak": true
      },
      {
        "second": 3,
        "action": "Transition to product reveal",
        "audioContent": "I've been using this for 30 days",
        "emotions": ["anticipation"],
        "retentionRisk": "low"
      }
    ],
    "viralAnalysis": {
      "score": 78,
      "retentionPrediction": 65,
      "mechanisms": ["curiosity_gap", "pattern_interrupt", "emotional_hook"],
      "emotionalTriggers": ["surprise", "FOMO"],
      "audioImpact": {
        "voiceTone": "energetic",
        "pacing": "fast",
        "hookEffectiveness": 85
      },
      "recommendations": [
        "Ajouter un hook textuel dans les 2 premi√®res secondes",
        "Augmenter le rythme des cuts apr√®s la 5√®me seconde",
        "Le ton vocal est efficace - maintenir l'√©nergie"
      ]
    },
    "assets": {
      "hooks": [
        "Tu ne vas pas croire ce qui se passe ensuite...",
        "J'ai test√© pendant 30 jours et voil√† le r√©sultat",
        "Personne ne parle de cette technique"
      ],
      "script": "Script optimis√© de 150 mots...",
      "captions": ["Caption avec emojis üî•"],
      "hashtags": ["#viral", "#fitness", "#transformation"],
      "callToAction": "Abonne-toi pour plus de tips!"
    }
  },
  "meta": {
    "framesAnalyzed": 2,
    "audioDurationSeconds": 35,
    "estimatedCostUsd": 0.0045,
    "pipeline": ["azure-speech", "phi-4-multimodal", "deepseek-r1", "deepseek-v3"]
  }
}
```

## üí∞ Optimisation des Co√ªts

| Param√®tre | Valeur | Raison |
|-----------|--------|--------|
| FPS | 1 | 1 frame/seconde suffit |
| MAX_FRAMES | 40 | Cap pour shorts 35s |
| scale_width | 512 | R√©duire taille images |
| Audio batch | Oui | $0.18/h vs temps r√©el |
| Phi-4 context | 128K | Tout en un seul appel |

### Comparaison des co√ªts

| Mod√®le | Co√ªt pour 1 short (35s) |
|--------|-------------------------|
| DeepSeek R1 | ~$0.002 |
| DeepSeek V3 | ~$0.001 |
| Phi-4 Multimodal | ~$0.001 |
| Azure Speech | ~$0.001 (35s = 0.01h) |
| **Total** | **~$0.005** |

## ‚ö†Ô∏è Points Critiques

1. **Ne jamais afficher le reasoning R1** - Les tags `<think>` sont extraits mais jamais montr√©s en prod
2. **Phi-4 vs Llama Vision** - Phi-4 est pr√©f√©r√©, Llama Vision reste en fallback
3. **Azure Speech batch** - Utiliser le mode batch pour les co√ªts, pas le temps r√©el
4. **Timeline analysis** - Activer `secondBySecond: true` pour l'analyse d√©taill√©e

## üîó Fichiers Cl√©s

- `lib/ai/content-trends/azure-inference-client.ts` - Client unifi√©
- `lib/ai/content-trends/azure-foundry-config.ts` - Configuration (mis √† jour pour Phi-4)
- `lib/ai/content-trends/ai-router.ts` - Routage intelligent (mis √† jour)
- `lib/ai/content-trends/phi4-multimodal-service.ts` - Service Phi-4 (√† cr√©er)
- `lib/ai/content-trends/audio-transcription-service.ts` - Service Azure Speech (√† cr√©er)
- `app/api/ai/content-trends/analyze/route.ts` - API endpoint
- `lib/ai/content-trends/video-processor.ts` - Extraction FFmpeg

## üîÑ Migration depuis Llama Vision

Si tu as d√©j√† Llama Vision configur√©:
1. Le syst√®me utilise automatiquement Phi-4 si configur√©
2. Llama Vision reste en fallback si `AZURE_PHI4_MULTIMODAL_ENDPOINT` n'est pas d√©fini
3. Aucune modification de code n√©cessaire - le routeur g√®re automatiquement

```typescript
// Le routeur choisit automatiquement
const model = getPreferredMultimodalModel(); // phi-4-multimodal ou llama-vision
```
